import { Request, Response } from 'express';
import { aiModelFactory } from '../services/aiModels/AIModelFactory';
import express from 'express';
import { IUser, AnalysisProgress } from '../types';
import { AnalyticsService } from '../services/AnalyticsService';
import { webSocketService } from '../services/WebSocketService';
import type { AIModelType } from '../types';
import fs from 'fs';

interface AuthenticatedRequest extends Request {
  user?: { 
    userId: string;
    email: string;
    role: string;
  };
}

export class AIAnalysisController {
  private analyticsService = new AnalyticsService();

  /**
   * Synchronous multi-AI analysis for immediate results
   */
  async analyzeWithMultipleAI(req: AuthenticatedRequest, res: Response): Promise<void> {
    try {
      const { selectedModels } = req.body;
      const userId = req.user?.userId;
      
      if (!req.file) {
        res.status(400).json({ error: 'No file uploaded' });
        return;
      }

      const file = req.file; // TypeScript now knows file is defined

      if (!userId) {
        res.status(401).json({ error: 'Authentication required' });
        return;
      }

      if (!selectedModels || selectedModels.length === 0) {
        res.status(400).json({ error: 'No AI models specified' });
        return;
      }

      // Debug logging for file processing
      console.log('üîç DEBUG: File processing started');
      console.log('üìÑ File details:', file);
      console.log('üìä File name:', file.originalname);
      console.log('üìè File size:', file.size);
      console.log('üß¨ File buffer length:', file.buffer?.length || 'No buffer');

      // Read file content (handle binary files gracefully)
      let content: string;
      try {
        // For memory storage, use buffer instead of file path
        if (file.buffer) {
          content = file.buffer.toString('utf-8');
        } else if (file.path) {
          content = fs.readFileSync(file.path, 'utf-8');
        } else {
          // Fallback to filename and metadata
          content = `File: ${file.originalname}, Type: ${file.mimetype}, Size: ${file.size} bytes`;
        }
      } catch {
        // For binary files, just use filename and metadata
        content = `File: ${file.originalname}, Type: ${file.mimetype}, Size: ${file.size} bytes`;
      }
      
      console.log('üìÅ Processing file:', {
        name: file.originalname,
        type: file.mimetype,
        size: file.size,
        contentLength: content.length,
        selectedModels,
        userId
      });
      
      // Generate unique analysis ID for progress tracking
      const analysisId = `analysis_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
      console.log(`üÜî Analysis ID: ${analysisId}`);

      // Initialize progress tracking with better calculation
      const modelProgressMap = new Map<string, number>();
      selectedModels.forEach((model: AIModelType) => modelProgressMap.set(model, 0));

      const progressUpdate = (modelType: string, progress: number, status: string) => {
        // Update the specific model's progress
        modelProgressMap.set(modelType, progress);
        
        // Calculate overall progress across all models
        const totalProgress = Array.from(modelProgressMap.values()).reduce((sum, prog) => sum + prog, 0);
        const overallProgress = Math.round(totalProgress / selectedModels.length);

        const progressData = {
          analysisId,
          fileId: req.file?.filename || 'unknown',
          userId: userId!,
          overallProgress,
          modelProgress: {
            [modelType]: {
              progress,
              status: status as 'pending' | 'processing' | 'completed' | 'error' | 'cancelled',
              lastUpdated: new Date()
            }
          } as Record<AIModelType, any>,
          status: overallProgress === 100 ? 'completed' as const : 'processing' as const,
          startTime: new Date()
        } as AnalysisProgress;

        console.log(`üìä Progress update:`, {
          analysisId,
          modelType,
          progress,
          overallProgress,
          status,
          timestamp: new Date().toISOString()
        });

        // Send via WebSocket - emit individual model progress
        try {
          webSocketService.emitModelProgress(userId!, analysisId, modelType as AIModelType, progress, status as 'pending' | 'processing' | 'completed' | 'error' | 'cancelled');
        } catch (error) {
          console.warn('‚ö†Ô∏è Failed to emit progress via WebSocket:', error);
        }
      };

      // Send initial progress
      selectedModels.forEach((model: AIModelType, index: number) => {
        progressUpdate(model, 0, 'pending');
      });

      // Process each model in parallel with progress tracking
      console.log('ü§ñ Starting analysis with models:', selectedModels);
      const analysisResults = await Promise.allSettled(
        selectedModels.map(async (model: AIModelType, index: number) => {
          try {
            console.log(`üîç Getting AI model: ${model}`);
            const aiModel = aiModelFactory.getModel(model);
            
            // Send progress update - starting analysis
            progressUpdate(model, 10, 'processing');
            console.log(`üìä Starting analysis with ${model} (${index + 1}/${selectedModels.length})`);
            
            const result = await aiModel.analyze(content, file);
            console.log(`‚úÖ Analysis completed for ${model}:`, result);
            
            // Send progress update - completed
            progressUpdate(model, 100, 'completed');
            console.log(`üìä Completed analysis with ${model} (${index + 1}/${selectedModels.length})`);
            
            return { model, result };
          } catch (error) {
            console.error(`‚ùå Analysis failed for ${model}:`, error);
            progressUpdate(model, 0, 'error');
            throw error;
          }
        })
      );

      console.log('üìä All analysis results:', analysisResults);

      // Build response in expected frontend format
      const aiResults: Record<string, any> = {};
      
      analysisResults.forEach((result, index) => {
        const model = selectedModels[index];
        if (result.status === 'fulfilled') {
          const analysis = result.value.result;
          
          // Transform the analysis result to match frontend expectations
          aiResults[model] = {
            model: model,
            status: 'completed',
            analysisType: 'content_analysis',
            confidence: analysis.confidence || 0.85,
            processingTime: analysis.processingTime || 1.5,
            timestamp: new Date(),
            results: {
              description: analysis.summary || 'Content analysis completed successfully. The AI model has processed the uploaded content and extracted key insights.',
              insights: analysis.keyPoints && analysis.keyPoints.length > 0 ? analysis.keyPoints : [
                'Content successfully processed and analyzed',
                'Key themes and concepts have been identified',
                'Text structure and composition evaluated'
              ],
              entities: analysis.topics?.map((topic: string, idx: number) => ({
                entity: topic,
                type: 'concept' as const,
                confidence: 0.8 + (idx * 0.05), // Vary confidence slightly
                context: `Key concept identified from content analysis`
              })) || [
                {
                  entity: 'Main Content',
                  type: 'concept' as const,
                  confidence: 0.9,
                  context: 'Primary content theme identified'
                }
              ],
              recommendations: [
                'Content has been analyzed successfully',
                'Review the detailed insights for key findings',
                analysis.complexity ? `Content complexity: ${analysis.complexity}` : 'Standard complexity level detected',
                analysis.readability ? `Readability score: ${analysis.readability}%` : 'Good readability maintained',
                'Consider exploring related topics for enhanced understanding'
              ].filter(Boolean)
            }
          };
        } else {
          // Include error information for failed models
          aiResults[model] = {
            model: model,
            status: 'error',
            analysisType: 'error',
            confidence: 0,
            processingTime: 0,
            timestamp: new Date(),
            results: {
              description: 'Analysis failed for this model. Please try again or use a different AI model.',
              insights: [`Analysis with ${model} encountered an error: ${result.reason || 'Unknown error'}`],
              entities: [],
              recommendations: [
                'Try uploading the file again',
                'Use a different AI model',
                'Check file format compatibility',
                'Contact support if the issue persists'
              ]
            }
          };
        }
      });

      // Track analytics
      await this.analyticsService.recordUserAnalytics({
        sessionId: `sync_${Date.now()}`,
        action: 'multi_ai_analysis_completed',
        resource: file.originalname,
        metadata: { 
          selectedModels, 
          mode: 'sync',
          successfulModels: Object.keys(aiResults).filter(k => aiResults[k].confidence > 0).length
        },
        userId
      });

      // Clean up file
      if (file.path && fs.existsSync(file.path)) {
        fs.unlinkSync(file.path);
      }

      console.log('üì§ Sending response to frontend:', { aiResults, analysisId });
      // Return in format expected by frontend
      res.json({ aiResults, analysisId });

    } catch (error) {
      console.error('Sync analysis error:', error);
      res.status(500).json({ error: 'Analysis failed' });
    }
  }
}
