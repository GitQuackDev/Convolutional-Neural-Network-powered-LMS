# Story 1.2: AI Model Service Infrastructure

## Status
Draft

## Story
**As a** developer,  
**I want** to implement a modular AI model service layer alongside existing CNN services,  
**so that** multiple AI models can analyze content without disrupting current CNN workflows.

## Acceptance Criteria
1. New AI service layer supports GPT-4, Claude, and Gemini integration with fallback mechanisms
2. Existing CNN analysis service continues to operate independently
3. AI model service includes rate limiting, error handling, and circuit breaker patterns
4. Service supports both synchronous and asynchronous analysis modes
5. Configuration allows enabling/disabling individual AI models without system restart

## Tasks / Subtasks
- [ ] Create base AI service infrastructure (AC: 1, 2)
  - [ ] Design AbstractAIService interface for consistent AI model integration
  - [ ] Implement AIServiceFactory with Strategy pattern for model selection
  - [ ] Create AIServiceManager for orchestrating multiple AI models
  - [ ] Implement fallback mechanism between AI services
- [ ] Implement specific AI model services (AC: 1)
  - [ ] Create GPT4Service with OpenAI API integration
  - [ ] Create ClaudeService with Anthropic API integration  
  - [ ] Create GeminiService with Google AI API integration
  - [ ] Implement unified response format across all AI services
- [ ] Add reliability and performance patterns (AC: 3)
  - [ ] Implement circuit breaker pattern for external API calls
  - [ ] Add rate limiting per AI service provider
  - [ ] Create retry logic with exponential backoff
  - [ ] Implement request timeout handling
- [ ] Support synchronous and asynchronous modes (AC: 4)
  - [ ] Create sync analysis methods for immediate responses
  - [ ] Implement async analysis with job queue and progress tracking
  - [ ] Add WebSocket notifications for async completion
  - [ ] Create progress tracking for long-running analysis
- [ ] Configuration and service management (AC: 5)
  - [ ] Create dynamic service configuration system
  - [ ] Implement hot-reload for AI service enable/disable
  - [ ] Add service health monitoring and status reporting
  - [ ] Create service metrics and performance monitoring
- [ ] Integration testing and validation (AC: 2)
  - [ ] Test AI services work independently of existing CNN service
  - [ ] Validate existing CNN functionality remains unaffected
  - [ ] Test fallback mechanisms under various failure scenarios
  - [ ] Performance test concurrent AI model usage

## Dev Notes

### Previous Story Insights
**Story 1.1 Context**: Database schema has been enhanced with new collections including `ai_analysis_results` which this story will utilize for storing multi-AI model outputs. The `AIAnalysisResults` model is available for storing consolidated insights from multiple AI providers.

### Data Models
**Source: Story 1.1 database schema and PRD requirements**

**Existing Schema to Utilize:**
- `AIAnalysisResults` model from Story 1.1 [Source: docs/stories/1.1.database-schema-enhancement.md]
- Fields: `cnnResults`, `gpt4Results`, `claudeResults`, `geminiResults`, `consolidatedInsights`
- Performance indexes already defined for efficient querying

**AI Service Response Format:**
```typescript
interface AIAnalysisResponse {
  modelName: string;
  modelVersion: string;
  confidence: number;
  processingTime: number;
  results: {
    textExtraction?: string;
    conceptIdentification?: string[];
    learningPathways?: string[];
    insights?: string;
    metadata?: Record<string, any>;
  };
  error?: string;
}
```

**AI Service Configuration:**
```typescript
interface AIServiceConfig {
  enabled: boolean;
  apiKey: string;
  baseUrl?: string;
  timeout: number;
  rateLimit: {
    requests: number;
    windowMs: number;
  };
  retryConfig: {
    maxRetries: number;
    backoffMs: number;
  };
}
```

### API Specifications
**Source: PRD Integration Strategy and backend architecture analysis**

**New API Endpoints:** [Source: PRD Integration Strategy]
- `POST /api/ai-models/analyze` - Multi-AI analysis endpoint
- `GET /api/ai-models/status` - Service health and availability
- `GET /api/ai-models/results/:id` - Async analysis results retrieval
- `POST /api/ai-models/analyze/async` - Asynchronous analysis initiation

**API Integration Patterns:** [Source: docs/backend-architecture.md]
- Follow existing Express.js 5.1.0 middleware stack patterns
- Maintain JWT authentication on all protected routes
- Use existing rate limiting and error handling middleware
- Integrate with Prisma client for database operations

**External API Integration:** [Source: docs/backend-architecture.md#AI/ML Integration Stack]
- Current pattern: Hugging Face (primary) + OpenRouter (fallback)
- New pattern: GPT-4, Claude, Gemini with circuit breaker failover
- Existing axios client patterns for external API communication

### Component Specifications
**Source: Backend architecture patterns and service layer design**

**Service Layer Architecture:** [Source: docs/backend-architecture.md#Architectural and Design Patterns]
- **Service Layer Pattern**: Centralized business logic with clear separation from controllers
- **Strategy Pattern**: Multiple AI providers with fallback capabilities (already established)
- **Factory Pattern**: Dynamic content processors based on analysis requirements

**Key Service Classes:**
```typescript
// Base interface for all AI services
abstract class AbstractAIService {
  abstract analyzeContent(content: Buffer, mimeType: string): Promise<AIAnalysisResponse>;
  abstract isAvailable(): Promise<boolean>;
  abstract getServiceInfo(): AIServiceInfo;
}

// Factory for creating AI service instances
class AIServiceFactory {
  static createService(type: 'gpt4' | 'claude' | 'gemini'): AbstractAIService;
}

// Manager for orchestrating multiple AI services
class AIServiceManager {
  async analyzeWithFallback(content: Buffer, mimeType: string): Promise<AIAnalysisResponse[]>;
  async getAvailableServices(): Promise<string[]>;
}
```

### File Locations
**Source: Current project structure analysis and backend architecture**

**New Files to Create:**
- `backend/src/services/aiModels/` - New directory for AI service implementations
  - `AbstractAIService.ts` - Base AI service interface
  - `GPT4Service.ts` - OpenAI GPT-4 integration
  - `ClaudeService.ts` - Anthropic Claude integration
  - `GeminiService.ts` - Google Gemini integration
  - `AIServiceFactory.ts` - Service factory implementation
  - `AIServiceManager.ts` - Service orchestration manager
  - `types.ts` - TypeScript interfaces and types
- `backend/src/controllers/aiModelsController.ts` - API endpoint controller
- `backend/src/routes/ai-models.ts` - Express routes for AI endpoints
- `backend/src/middleware/circuitBreaker.ts` - Circuit breaker middleware
- `backend/src/config/aiServices.ts` - AI service configuration

**Files to Modify:**
- `backend/src/server.ts` - Add new AI models routes
- `backend/package.json` - Add new dependencies (OpenAI, Anthropic SDKs)
- `backend/.env.example` - Add AI service API key placeholders

### Testing Requirements
**Source: PRD Testing Integration Strategy**

**Testing Framework:** Vitest + Testing Library (existing setup) [Source: docs/prd.md#Testing Integration Strategy]

**Required Tests:**
1. **Unit Tests** (`backend/tests/services/aiModels/`)
   - Test each AI service implementation independently
   - Mock external API calls for deterministic testing
   - Test circuit breaker and retry logic
   - Validate service factory pattern

2. **Integration Tests** (`backend/tests/integration/`)
   - Test AI service manager orchestration
   - Verify database integration with AIAnalysisResults model
   - Test API endpoints with authentication
   - Validate existing CNN service remains functional

3. **Performance Tests** (`backend/tests/performance/`)
   - Test concurrent AI model analysis (NFR1 compliance)
   - Benchmark memory usage during multi-model analysis
   - Test rate limiting and timeout handling

**Mocking Strategy:**
- Mock external AI API responses for consistent testing
- Use test database for integration testing
- Mock WebSocket connections for async testing

### Technical Constraints
**Source: PRD Non-Functional Requirements and backend architecture**

**Performance Requirements:** [Source: docs/prd.md#NFR1]
- Memory usage must not exceed 30% increase during concurrent multi-model analysis
- Maintain existing CNN processing performance characteristics
- Response times must remain within acceptable limits

**Technology Constraints:** [Source: docs/backend-architecture.md#Tech Stack]
- Node.js 20.11.0 LTS compatibility
- Express.js 5.1.0 integration
- TypeScript strict mode compliance
- Existing middleware stack integration (Helmet, CORS, rate limiting)

**Security Requirements:**
- API keys stored securely in environment variables
- External API communication over HTTPS only
- Input validation for all AI analysis requests
- Rate limiting to prevent API abuse

**Reliability Requirements:**
- Circuit breaker pattern for external API failures
- Graceful degradation when AI services are unavailable
- Proper error handling and logging for debugging
- Fallback to existing CNN analysis if all AI services fail

### External Dependencies
**New NPM Packages Required:**
```json
{
  "openai": "^4.x.x",
  "@anthropic-ai/sdk": "^0.x.x", 
  "@google-ai/generativelanguage": "^2.x.x",
  "opossum": "^8.x.x",
  "p-retry": "^6.x.x"
}
```

**Environment Variables:**
```bash
# AI Service API Keys
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
GOOGLE_AI_API_KEY=your_google_ai_api_key

# AI Service Configuration
AI_SERVICES_ENABLED=gpt4,claude,gemini
AI_REQUEST_TIMEOUT=30000
AI_MAX_RETRIES=3
```

## Testing

### Testing Standards
**Source: PRD Testing Integration Strategy**

**Test Framework:** Vitest + Testing Library (existing setup)
**Test File Location:** `backend/tests/` directory following current patterns
**Testing Patterns:** Maintain current testing patterns while adding AI service-specific tests

**Required Testing Approaches:**
1. **Unit Tests:** Individual AI service functionality and patterns
2. **Integration Tests:** Service orchestration and database integration  
3. **Performance Tests:** Concurrent analysis and memory usage validation
4. **External API Tests:** Mocked external service integration testing

**AI Service Testing Requirements:**
- Mock external AI API calls for deterministic testing
- Test circuit breaker behavior under failure conditions
- Validate fallback mechanisms work correctly
- Test configuration hot-reload functionality

**Performance Testing Specific Requirements:**
- Benchmark concurrent AI model analysis
- Monitor memory usage during multi-model operations (30% limit)
- Test rate limiting effectiveness across multiple AI services
- Validate timeout handling for slow AI responses

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-08-31 | 1.0 | Initial story creation with comprehensive AI service infrastructure context | Bob (Scrum Master) |
